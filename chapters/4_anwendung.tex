\chapter{Anwendung des Frameworks auf jadice flow}
\label{chap:anwendung}

In diesem Kapitel wird ein Architektur-Refactoring am Produkt \emph{jadice flow} geplant und in theoretischer Ebene durchgeführt.
Dazu werden \acrfull{mmf} und \acrfull{arh} verwendet, welche den Prozess sowie auch dieses Kapitel in drei Phasen unterteilen.
Eine genauere Beschreibung des Frameworks ist in \cref{sec:mmf} zu finden.
Sehr abstrahiert und vereinfacht bestehen die Phasen aus den folgenden Aktivitäten:
\begin{itemize}
	\item In \cref{sec:durchführung-phase1} wird die Durchführung der ersten Phase in Form eines Architekturreviews mit den wichtigsten Stakeholdern beschreiben.
	\item Im Rahmen der zweiten Phase wird in \cref{sec:durchführung-phase2} nach adäquaten Migrationsstrategien gesucht.
	\item In \cref{sec:durchführung-phase3} wird in der dritten Phase nach profitablen Patterns und Best Practices gesucht.
\end{itemize}

\section{Phase 1 - Systemverständnis}
\label{sec:durchführung-phase1}

Das Ziel dieser Phase ist es, ein Verständnis des Systems aufzubauen.
Das ist zum einen auf Seite der Stakeholder wichtig.
Diese sollten spätestens nach dieser Phase wissen, welche \acrfullpl{qa} besonders wichtig für das System sind.
Zum anderen sollte der \gls{arh} nach dieser Phase durch Eingabe der \glspl{qa} ein Verständnis des Systems erlangen, das er in den nächsten Phasen zur Unterstützung der Entwickler bei Migration verwenden kann.
Um das Systemverständnis zu erlangen, wurde in dieser Phase am 6. November 2023 ein Architekturreview wie in \cref{sec:methodik-architekturreview} beschrieben durchgeführt und am 14. November 2023 fortgesetzt.
Teilgenommen haben vier Softwareentwickler beziehungsweise -Architekten, der \acrlong{po} und der Autor selbst in der Rolle des Moderators.
Leider war es nicht möglich, einen Kunden oder Nutzer des Produkts als Stakeholder für dieses Review zu organisieren.
Da das Entwicklungsteam jedoch regelmäßigen Kontakt mit Kunden hat, haben sie bestmöglich versucht, auch die Interessen der Kunden zu repräsentieren.
Der Zeitrahmen für diese Besprechung war durch die Planung, die in \cref{sec:methodik-architekturreview} beschrieben ist, auf zwei Stunden festgelegt.
Im Folgenden wird der Begriff \emph{Schritt} immer auf den Schritt nach der beschriebenen Methodik bezogen und nicht auf Schritte des \gls{arh}.
Da der erste Schritt dabei lediglich eine Einführung in die Methodik ist, wird direkt mit den Ergebnissen des zweiten Schritts fortgefahren.

\subsection{Priorisierung der Qualitätsattribute (Schritt 2)}

Im zweiten Schritt werden die gewünschten \glspl{qa} des Systems gesammelt.
Im Rahmen dessen sollte jeder Teilnehmer seine Einschätzung darüber, welche die wichtigsten drei Sub-\glspl{qa} sind, per Nachricht mitteilen.
Die daraus resultierende Anzahl von Stimmen pro Sub-\gls{qa} ist in \cref{fig:qas-priority} dargestellt.
% \input{tables/qas-priority}
\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{qas-priority.drawio}
	\caption[Umfrageergebnisse wichtigste (Sub-) QAs im Architekturreview]{
		Umfrageergebnisse bei der Suche nach den wichtigsten (Sub-) QAs im Architekturreview in Phase 1 der Migration mit N = 5.
	}
	\label{fig:qas-priority}
\end{figure}
Dabei ist auffällig, dass auch  \acrlongpl{qa} Stimmen erhalten haben.
Das liegt daran, dass nicht alle Teilnehmer der Umfrage sich auf Sub-\glspl{qa} beschränkt haben, sondern auch die \glspl{qa} \emph{Performance}, \emph{Reliability}, \emph{Maintainability} und \emph{Portability} genannt wurden.
Auch wenn das nicht so vorgesehen war, wurde davon abgesehen, die Teilnehmer darauf hinzuweisen und es zu korrigieren, da die Ergebnisse nicht direkt zur Priorisierung der Attribute führen müssen.
Stattdessen wurden die Umfrageergebnisse und als vage Basis für eine freiere Diskussion über die Priorisierung verwendet.
Deren Ergebnis war die schlussendliche Platzierung der sechs wichtigsten Sub-\glspl{qa}.
Im Folgenden werden diese erläutert.
\begin{enumerate}
	\item \textbf{\emph{Scalability}} ist ein \gls{qa} ohne Subattribute, es wird allerdings auch oft \emph{Performance} zugeordnet. Nach \Citet{master-daniel-koch} gibt es an, wie effizient ein System skaliert werden kann~\cite{koch-scalability-1}. Im Kontext von Microservices ist dabei vor allem die horizontale Skalierung relevant, welche die Skalierung über mehrere Instanzen von der Microservices beschreibt~\cite{koch-scalability-2}.
	\item \textbf{\emph{Modularity}} ist ein Subattribut von \emph{Maintainability}. Nach \Citet{master-daniel-koch} gibt es an, wie gut die Komplexität des Systems auf verschiedene Komponenten verteilt ist~\cite{ISO-25010}.
	\item \textbf{\emph{Time Behavior}} ist Subattribut von \emph{Performance}. Nach \Citet{master-daniel-koch} gibt es an, wie schnell ein System oder Teile eines Systems ankommende Anfragen beantworten~\cite{ISO-25010,koch-time-behavior-1,koch-time-behavior-2}.
	\item \textbf{\emph{Deployability}} ist ein Subattribut von \emph{Portability}. Nach \Citet{master-daniel-koch} gibt es an, wie einfach und schnell ein Produkt gebaut und ausgeliefert werden kann~\cite{koch-scalability-1,koch-deployability}.
	\item \textbf{\emph{Fault Tolerance}} ist Subattribut von \emph{Reliability}. Nach \Citet{master-daniel-koch} gibt es an, inwieweit ein System wie erwartet funktionieren kann, obwohl in Teilen des Systems Fehler passieren~\cite{ISO-25010,koch-fault-tolerance}.
	\item \textbf{\emph{Recoverability}} ist ebefalls Subattribut von \emph{Reliability}. Nach \Citet{master-daniel-koch} gibt es an, inwieweit ein System nach einem Ausfall wieder den Zustand vor dem Ausfall herstellen kann, sowie Daten erhalten kann~\cite{ISO-25010}.
\end{enumerate}
Die genaue Anzahl der wichtigsten (Sub-)Attribute war nicht vor dem Termin definiert, sondern wurde ebenfalls von der Gruppe diskutiert (in einem späteren Schritt) und gemeinsam auf sechs festgelegt.
Der gesamte Prozess der Umfrage und anschließender Diskussion, um die Prioritäten der Attribute zu setzen, dauerte etwa 15 Minuten und entsprach somit der geplanten Zeit.

\subsection{Szenarienerhebung (Schritt 3)}

Im nächsten Schritt war die Aufgabe, für jedes dieser (Sub-) \glspl{qa} zwei Szenarien zu definieren, die das Attribut gut und auf verschiedene Arten beschreiben.
Es wurde keine spezielle Methodik angewandt, sondern in einer offenen Diskussion der gesamten Gruppe von verschiedenen Teilnehmern nacheinander für die Attribute Szenarien vorgeschlagen und anschließend überarbeitet oder teilweise auch abgelehnt.
Diese Szenarien wurden in Form eines \emph{Utility Trees}, beschrieben in \cref{sec:atam-saam-svahnberg}, festgehalten.
Nach einer Diskussionszeit von etwa 35 Minuten ergaben sich die Szenarien, die in  \cref{fig:scenarios} abgebildet sind.

\begin{figure}
	\centering
	\includegraphics[angle=270,width=\textwidth]{scenarios.drawio}
	\caption[Utility Tree mit im Architekturreview ermittelten Qualitätsanforderungen und Szenarien]{
		Der Utility Tree mit Szenarien, die aus dem in Phase 1 durchgeführten Architekturreview resultieren.
		Von oben nach unten enthält der Baum folgende Elementarten: [1] Wurzel (ohne Bedeutung), [2] \gls{qa}, [3] Subattribut, [4] Beurteilung des Szenarios hinsichtlich Wichtigkeit und technischer Schwierigkeit, [5] Szenariobeschreibung.
	}
	\label{fig:scenarios}
\end{figure}

Wie aus der Abbildung ersichtlich, wurden für jedes (Sub-)Attribut mit Ausnahme von \emph{Fault Tolerance} und \emph{Recoverability} jeweils zwei Szenarien erstellt.
Dies hat zwei Gründe.
Zum einen wurden für diese Subattribute keine ausreichend unterschiedlichen Szenarien gefunden.
Zum anderen wurde in diesem Fall ein Szenario als ausreichend betrachtet, da Reliability das einzige QA ist, für das mehrere Subattribute eingeschlossen wurden.
Darüber hinaus ist es das am geringsten priorisierte Attribut.

Um zusätzliche eine Priorisierung der Szenarien vornehmen zu können, sieht der \gls{arh} jeweils eine dreistufige Bewertung der Szenarien hinsichtlich ihrer Wichtigkeit und ihrer technischen Schwierigkeit vor.
Diese wurde im nächsten Schritt vorgenommen.
Dabei haben die Teilnehmer die Szenarien der Reihe nach betrachtet und ihre Wichtigkeit und technische Schwierigkeit diskutiert.
Nicht immer waren alle anfangs einer Meinung, doch schlussendlich konnte nach etwa 35 Minuten für jedes Szenario Konsens gefunden werden und diese Phase abgeschlossen werden.

Da die Vollendung des dritten Schritts nach \Citet{SVAHNBERG20071893} länger gedauert hat als geplant, waren am Ende dieses Termins nur noch wenige Minuten übrig.
Für alle Sub-Schritte dieses Schritts zusammen waren nur 60 Minuten eingeplant, das Sammeln der Szenarien und das Bewerten dieser (also zwei von drei Sub-Schritten) nahm jedoch schon 70 Minuten in Anspruch.
Daher wurde entschieden, die restlichen Punkte in einer weiteren Sitzung in der nächsten Woche zu bearbeiten.

Am 14. November 2023 wurde dann der zweite Teil des Architekturreviews durchgeführt.
Anwesend waren dieselben Personen.
Da bei diesem Treffen ein neuer Blickwinkel auf die Thematik möglich gewesen wäre, wurden zunächst zehn Minuten darauf verwendet, die bereits vorhandenen Szenarien und die Bewertung dieser hinsichtlich Wichtigkeit oder technischer Schwierigkeit erneut zu überprüfen und Raum für mögliche Änderungen zu schaffen.
Es wurden jedoch keine Änderungen vorgenommen und die Teilnehmer bestätigten lediglich erneut die bereits vorhandenen Szenarien.

Anschließend wurde wie geplant damit fortgefahren, den Szenarien weitere \glspl{qa} zuzuordnen.
Oft können trotz der Erstellung eines Szenarios für ein bestimmtes \gls{qa} weitere \glspl{qa} damit assoziiert werden.
Deswegen wurde erneut jedes Szenario betrachtet und diskutiert, welche weiteren (Sub-) Attribute darauf zutreffen könnten.
Nach etwa 15 Minuten Diskussion waren alle Szenarien behandelt und für jedes einstimmig geklärt, welche weiteren Subattribute damit assoziiert werden.
Diese werden folgend im Bezug auf die jeweiligen Szenarios sekundäre \glspl{qa} genannt, wohingegen ein \gls{qa}, für das das Szenario erstellt wurde, primäres \gls{qa} genannt wird.
Die resultierenden Assoziationen sind in der \cref{tab:scenarios} aufgeführt.

\input{tables/scenarios}

Damit wurde der für die Extraktion der Qualitätsanforderungen des Systems relevante Teil des Architekturreviews abgeschlossen.
Zusammenfassend kann gesagt werden, dass die Phase größtenteils wie geplant durchgeführt werden konnte.
An einigen Stellen sind die verschiedene Schritte etwas verschmolzen oder es wurde kurz zu vorherigen Schritten zurückgesprungen, was jedoch vollkommen normal ist und auch beispielsweise in \gls{atam} von \Citet{kazman_2000} als gängige Praxis beschrieben wird.
So wurde in diesem Fall beim Erstellen der Szenarien entschieden, bis zu welcher Priorität die \glspl{qa} mit Szenarien versehen werden, obwohl die Wahl der Anzahl der wichtigsten tatsächlich für den vorherigen Schritt geplant war.
Die Beteiligung der Teilnehmer war komplett ausgeglichen, aber jeder hat regelmäßig etwas beigetragen.
Es kann angenommen werden, dass das Ergebnis von allen Teilnehmern ausreichend geformt wurde und dass keine einseitige Beeinflussung vorliegt.

\subsection{Architekturanalyse anhand Szenarien (Schritt 5)}

Neben der Erfassung der wichtigsten Szenarien und \glspl{qa} hatte die Fokusgruppe als sekundäres Ziel, die Wahl der Architektur für \emph{jadice flow} zu bewerten.
Diese Bewertung wurde ebenfalls in der zweiten Sitzung des Architekturreviews durchgeführt und entspricht dem vierten Schritt \emph{Assessment} der ersten Phase des \gls{arh}.
Da dieser Schritt jedoch noch nicht im Werkzeug implementiert ist, wurde er manuell durchgeführt.
Im Gegensatz zu vorherigen Schritten der ersten Phase ist dieser nicht relevant für die nächsten Phasen, weshalb es kein Problem ist, den \gls{arh} hierbei nicht zu verwenden.
In diesem Schritt wurde die Frage diskutiert, ob eine \acrlong{msa} individuell für jedes Szenario vorteilhaft ist im Vergleich zu einer monolithischen Architektur.
Hierbei wurde in drei Stufen unterschieden:
\begin{itemize}
	\item \advantage\hspace*{0.1cm}: Dieses Szenario profitiert wesentlich mehr von einer \gls{msa} als von einer monolithischen Architektur.
	\item \disadvantage\hspace*{0.1cm}: Dieses Szenario profitiert wesentlich mehr von einer monolithischen Architektur als von einer \gls{msa}.
	\item \hspace*{0.27cm}-\hspace*{0.27cm}: Dieses Szenario profitiert in verschiedenen Punkten sowohl von einer \gls{msa} als auch von einer monolithischen Architektur, wobei sich beide Seiten ungefähr gleichen.
\end{itemize}
Bei Szenarios wie beispielsweise den des \emph{Scalability} Attributs war es nicht schwer, Konsens zu finden, da die Möglichkeit der Skalierung auf Service-Ebene einer der größten Vorteile von \glspl{msa} gegenüber Monolithen ist.
In anderen Fällen jedoch war es schwieriger, abzuwägen, ob die Vorteile einer \gls{msa} oder die eines Monolithen überwiegen.
Schlussendlich konnte jedoch in Form von offener  Diskussion für jedes Szenario einstimmig geklärt werden, welche der drei Optionen gewählt werden sollte, sodass keine Abstimmungen notwendig waren.

Das Ergebnis dieser Einschätzungen ist ebenfalls in \cref{tab:scenarios} zu sehen.
Insgesamt wurde eine \gls{msa} in fünf Fällen als vorteilhaft und nur in drei Fällen als unvorteilhaft bewertet.
Des Weiteren ist zu beachten, dass die Szenarien in der Tabelle nach der Wichtigkeit der primären \glspl{qa} sortiert sind und die obersten beiden Szenarien die Hauptfaktoren für die Migration zu einer Microservices-Architektur waren.
Durch die Überlegenheit der \gls{msa}-Favorisierungen und dass diese verstärkt bei den wichtigsten \glspl{qa} vorliegen, kann diese Auswertung die Entscheidung zur Migration zu einer \gls{msa} bestätigen.

Nachdem nun erwünschte Szenarien und \glspl{qa} erhoben wurden und die Wahl einer \gls{msa} für \emph{jadice flow} bekräftigt wurde, kann mit Phase 2 fortgefahren werden.

\section{Phase 2 - Strategieplanung}
\label{sec:durchführung-phase2}

In dieser Phase wird die Suchfunktion des \gls{arh} verwendet, um eine geeignete Migrationsstrategie zu finden.
Diese Suche wird dabei von zwei Faktoren beeinflusst: dem Ergebnis der Phase 1 in Form von \glspl{qa} beziehungsweise Szenarien und zusätzlichen Filtern, die vor der Suche konfiguriert werden müssen.
Im Folgenden werden diese Filter konfiguriert, die Suche mit ihnen und den \glspl{qa} aus Phase 1 durchgeführt und die Ergebnisse dessen analysiert.
Abschließend wird dadurch eine Migrationsstrategie für diese Fallstudie ausgewählt.

\subsection{Filterselektion}
\label{sec:filterselektion}

Um bei der Suche nach Migrationsmethoden möglichst Ergebnisse zu erhalten, die dem Zielsystem und den Vorstellungen der Architekten entsprechen, bietet der \gls{arh} neben der Konfiguration der Szenarien eine weitere Möglichkeit, die Ergebnisse zu filtern und zu sortieren:
Die Filter, die in diesem Abschnitt konfiguriert werden.
Der \gls{arh} bietet insgesamt 71 verschiedene Filter aus fünf Kategorien und 16 Unterkategorien an.
Für jede der Eigenschaften aus \cref{tab:phase2-all-filter} kann eine dieser drei Präferenzen angegeben werden:
\begin{itemize}
	\item \textbf{Include:} Methoden, die die jeweilige Eigenschaft besitzen, werden höher platziert.
	\item \textbf{Neutral:} Ob Methoden die jeweilige Eigenschaft besitzen oder nicht, wirkt sich nicht auf ihre Platzierung aus.
	\item \textbf{Exclude:} Methoden, die die jeweilige Eigenschaft nicht besitzen, werden höher platziert.
\end{itemize}
Es wird empfohlen, präferierte Eigenschaften auf \emph{include} zu setzen und die anderen auf der Standardeinstellung \emph{neutral} zu belassen und die Einstellung \emph{exclude} nicht zu verwenden, da der Ausschluss einer Eigenschaft typischerweise nicht das Ziel ist.
Wenn eine Eigenschaft nicht erwünscht ist, kann es in vielen Fällen möglich sein, die Methode minimal abzuändern oder einen Teil der Methode zu ignorieren.
Dadurch sollte eine solche unerwünschte Eigenschaft trotzdem kein Problem darstellen und die Methode nicht schlechter bewertet werden.

\input{tables/all-filter}

Für die Wahl der Filter für das Refactoring von \emph{jadice flow} wurden alle Filter betrachtet und die wichtigsten für diesen Fall ausgewählt (\cref{feldnotiz:1,feldnotiz:2}).
Ausgewählt bedeutet folgend in diesem Kontext immer, dass der entsprechende Filter auf \emph{include} gesetzt wurde.
Die Option \emph{exclude} wurde nicht verwendet.
Dabei wurden die ausgewählten Filter in zwei Prioritätsstufen unterteilt (\cref{feldnotiz:3}).
Filter mit Priorität 1 sind dabei die höher priorisierten Filter.
Diese Unterteilung ermöglicht es, Suchvorgänge mit verschiedenen Filterkonfigurationen durchzuführen, um mehrere mögliche Anwendungsfälle zu testen.
Wie aus \cref{tab:phase2-search-description} hervorgeht, wird eine Suche mit allen Filtern, eine Suche mit den höher priorisierten Filtern und eine Suche ausschließlich mit den \glspl{qa} durchgeführt.
\input{tables/search-description}
Dies soll einerseits zu besseren Ergebnissen führen und andererseits die Filterfunktion untersuchen. 
Die ausgewählten Filter sind in \cref{tab:phase2-all-filter} markiert und in \cref{tab:phase2-selected-filter} einzeln aufgelistet.

Die \glspl{sp} wurden als besonders wichtig erachtet, weshalb viele Filter dieser Kategorie aufgenommen wurden.
Bei den anderen Kategorien wurden nur die nötigsten Filter verwendet, da eine zu große Anzahl an Filtern die Aussagekraft verringern könnte.
Vor allem \emph{Process Preferences} und \emph{Usability Preferences}, also Eigenschaften, die die interne Funktionsweise der Methode betreffen, wurden als relativ unwichtig und vor allem auch unabhängig vom betreffenden System eingestuft.
Daher wurde als einziger Filter in diesen Kategorien bei der Prozess-Strategie die Eigenschaft \emph{Refactoring} gewählt.
Da bereits eine \gls{msa} vorliegt wären die anderen Optionen wie \emph{Rewrite/Rebuild} wenig sinnvoll.
Die \emph{Input Preferences} und \emph{Output Preferences} hängen ebenfalls stark vom System ab und betreffen die Ausgangslage sowie das gewünschte Ergebnis.
Deswegen wurden sie neben den \glspl{sp} ebenfalls als besonders bedeutend eingestuft. 

\input{tables/selected-filter}

Die Ergebnisse der Suchen mit diesen Filtern werden im Folgenden erläutert.

\subsection{Suchergebnisbetrachtung}
\label{sec:phase2-ergebnisdurchsicht}

Die im vorherigen Abschnitt gewählten Filter werden nun angewendet, um geordnete Listen von Ergebnissen zu erhalten. 
Ziel dieser Fallstudie ist es, eine Migrationsmethode zu finden und durchzuführen. 
Dabei sollen nur eine begrenzte Anzahl an Optionen manuell betrachtet werden, da der \gls{arh} eine großflächigere Betrachtung der Verfahren vermeiden können sollte.
Um eine realistisch begrenzte Auswahl der Verfahren für die detaillierte Betrachtung sicherzustellen, wurden folgende Schritte unternommen:
Es wurden drei Suchen mit den im vorherigen Abschnitt erläuterten Filtern durchgeführt.
Die ersten fünf Ergebnisse jeder Suche wurden in die Auswahl aufgenommen.
Außerdem wurden alle dem fünftem Ergebnis folgenden Ergebnisse übernommen, die die gleiche Anzahl an Übereinstimmungen aufweisen wie das fünfte.
Dadurch soll die Anzahl an Übereinstimmung als einziges Kriterium für die Ordnung und Auswahl sichergestellt werden, damit andere Faktoren wie eine mögliche lexikalische Ordnung oder Ordnung nach Identifikationsnummer keinen Einfluss erhalten.
Die Ergebnisse sind in \cref{tab:phase2-filter-results} zu sehen.
\input{tables/filter-results}

Um eine Ordnung der Ergebnisse aller Suchen zu erhalten, wurde eine Metrik für die Gesamtwertung jedes Verfahrens erstellt.
Diese wird mit $m_{gesamt}$ wie folgt berechnet:
\[
m_{gesamt} = \frac{2 \cdot m_{Wichtigste \text{ } Filter} + 1,5 \cdot  m_{Alle \text{ } Filter} +  m_{Keine \text{ } Filter}}{4,5}
\]
wobei $m_{gesamt}$ die gesamte prozentuale Übereinstimmung darstellt, also das Ergebnis, das in \cref{tab:phase2-ranking} angezeigt wird.
$m_{Wichtigste \text{ } Filter}$, $m_{Alle \text{ } Filter}$ und $m_{Keine \text{ } Filter}$ sind dabei die prozentualen Matches, die in \cref{tab:phase2-filter-results} vorne stehen.
Das wären beispielsweise im Fall \Citet{arh-result-no-filter-1} $m_{Wichtigste \text{ } Filter} = \frac{14}{26} \approx 53,8\%$.
Es wurde eine Gewichtung gewählt, sodass die Matches der Suche mit den wichtigsten Filtern doppelt so stark und die mit allen Filtern 1,5-fach gewichtet werden im Vergleich zu den Matches der Suche ohne Filter.
Diese Gewichtung wurde so konzipiert, dass die Suchen ungefähr proportional zu Gewichtung realistischste Eingaben abbilden.
Damit soll die Relevanz der Suche in die Metrik einfließen.
Die Ordnung der Ergebnisse nach dieser Metrik ist in \cref{tab:phase2-ranking} zu sehen.
\input{tables/phase2-result-matches}

In dieser Reihenfolge werden die einzelnen Methoden im Folgenden kurz zusammengefasst und anschließend ihre Anwendbarkeit verglichen.

\subsubsection{Ergebnis 1: \citetitle{arh-result-no-filter-1}  \cite{arh-result-no-filter-1}}

In dieser Arbeit stellen \citeauthor{arh-result-no-filter-1} ein konzeptuelles Evolutions-Framework vor, das mithilfe von Trans\-for\-ma\-tions\-re\-geln die Migration von Legacy-Systemen zu Microservices unterstützen soll.
Das Vorgehen ist in drei Teilabschnitte unterteilt. 

% TODO get copyright for this
%\begin{figure}[!h]
%	\centering
%	\includegraphics[width=0.7\textwidth]{arh-result-1}
%	\caption[Konzeptuelles Framework von \Citet{arh-result-no-filter-1}]{
%		Konzeptuelles Framework von \Citet{arh-result-no-filter-1}.
%	}
%	\label{fig:arh-result-1}
%\end{figure}
Das \emph{Core System} besteht aus dem Legacy System und einer Auswahl von Transformationsregeln.
Diese können aus der Sammlung der vorgestellten Regeln der Autoren stammen, aber auch durch eigene ergänzt werden.
Daraus wird ein \emph{Middle Layer} erstellt, indem diese Transformationsregeln auf das Legacy-System angewendet werden.
Das Resultat dessen ist eine Dekomposition des Systems in Microservices.
Das \emph{Target System} kann dann entwickelt werden, indem die erhaltenen Services in einer hybriden Cloud, bestehend aus einer privaten und einer öffentlichen Cloud, umgesetzt werden.

Diese Vorgehensweise wurde auch in einer Fallstudie überprüft.
Bei dieser wurde ein öffentliches GitHub Projekt als Legacy System verwendet, welches dann mit drei der vorgestellten Trans\-for\-ma\-tions\-regeln zu einer \gls{msa} migriert wurde.
Durch die Architekturänderung konnte das \emph{Coupling} reduziert, sowie die Modularität, Skalierbarkeit und Anfragezeit verbessert werden.
Die Autoren weisen jedoch darauf hin, dass diese Ergebnisse nur bedingt aussagekräftig sind und noch an mittleren bis großen Industriesystemen reproduziert werden müssen.

%Insgesamt wirkt die Methode sehr abstrakt und oberflächlich und wenig spezifisch.

\subsubsection{Ergebnis 2: \citetitle{arh-result-no-filter-3} \cite{arh-result-no-filter-3}}

In diesem Artikel stellen die Autoren eine Methode vor, die sie anhand einer Fallstudie mit Barcelonas Fahrradvermietungssystem \emph{Bicing} erklären und validieren.
Die Methode besteht aus drei übergeordneten Schritten.
Im ersten Schritt werden die Eingaben für die Methode in Form von \glspl{bp} gesammelt und logisch verbunden, beispielsweise in Form eines \gls{bpmn}-Diagramms.
Für die Bestandteile dieser Eingabe (genannt Aktivitäten) werden im zweiten Schritt Abhängigkeiten in drei verschiedenen Formen analysiert: \emph{control dependencies}, \emph{data dependencies} und \emph{semantic dependencies}.
Eine \emph{control dependency} beschreibt eine Abhängigkeit zweier Aktivitäten im Kontrollfluss, also dass eine der Aktivitäten mit hoher Wahrscheinlichkeit auf die andere folgt.
Eine \emph{data dependency} bildet einen Datenfluss zwischen den Ein- oder Ausgaben zweier Aktivitäten ab.
Eine \emph{semantic dependency} gibt die Verwandtschaft des Zwecks zweier Aktivitäten an.
Die Messung solcher Abhängigkeiten kann anhand der Ähnlichkeit der Namen der Aktivitäten erfolgen.

Schließlich wird die \emph{Machine Learning}-Technik \emph{Collaborative Clustering} verwendet, um die Menge von Aktivitäten in Gruppen (genannt \emph{Cluster}) zu unterteilen.
Die Autoren haben dazu den \gls{hac}~\cite{hierarchical-agglomerative-algorithm} erweitert.
Das Ziel besteht darin, dass Aktivitäten innerhalb eines Clusters eine hohe Kohäsion aufweisen und Aktivitäten zwischen verschiedenen Clustern lose gekoppelt sind, um potenziell Microservices aus den Clustern zu generieren.

% TODO Einfügen der Übersicht: TODO get license: https://www.sciencedirect.com/science/article/pii/S1383762121001442?via%3Dihub

\subsubsection{Ergebnis 3: \citetitle{arh-result-no-filter-2} \cite{arh-result-no-filter-2}}

\citeauthor{arh-result-no-filter-2} beschreiben die Methode \gls{mb}, ein semiautomatisches Modell zur Bewertung der Granularität einer \gls{msa}.
Im Gegensatz zu den meisten anderen  im Rahmen dieser Thesis betrachteten Methoden liegt hier der Kontext eines Refactorings statt einer Migration vor.
Es wird als Ausgangsarchitektur eine \gls{msa} erwartet, genau wie es bei \jf der Fall ist.
Als Eingabe werden \emph{User Stories} aus dem \emph{Product Backlog} oder der Release-Planung verwendet.
Diese werden in Kombination mit den Metriken \emph{Coupling}, \emph{Kohäsion}, \emph{Granularität}, \emph{semantische Ähnlichkeit} und \emph{Komplexität} von einem \emph{Genetic Algorithm} zu Microservices-Kandidaten dekomponiert.
\gls{mb} bietet außerdem eine Visualisierung dieses Ergebnisses an, wobei die Microservices unter den genannten Metriken betrachtet werden können.

Diese Methode wurde in drei Fallstudien überprüft, bei einer davon handelt es sich um eine industrielle Anwendung.

% TODO Einfügen der Übersicht: TODO get license: https://ieeexplore.ieee.org/document/9519691

\subsubsection{Ergebnis 4: \citetitle{arh-result-no-filter-4} \cite{arh-result-no-filter-4}}

In diesem Artikel beschreiben \citeauthor{arh-result-no-filter-4} eine automatisierte Methode mit zugehörigem Werkzeug \emph{MicroRefact}, mit dem Java-Monolithen in funktional äquivalente Applikationen bestehend aus Microservices umgewandelt werden können.
Als Eingabe der Methode fungieren der Quellcode des Monolithen sowie ein Vorschlag für die Microservices-Aufteilung. Dieser wird in Form von Klassen pro Microservice angegeben, wobei jede Klasse nur einem Microservice angehören kann.
In der ersten Phase des Werkzeugs, der \emph{Information Extraction}, nutzt es den Quellcode, um strukturelle Informationen zu gewinnen und Abhängigkeiten zwischen den gegebenen Microservices ermitteln.
Diese Abhängigkeiten werden in der zweiten Phase (\emph{Database Refactoring}) dann verwendet, um die Entitäten des Systems und die Beziehungen zwischen diesen zu identifizieren und ein Refactoring der Beziehungen anzustoßen.
In der letzten Phase werden die strukturellen Informationen und Abhängigkeiten zwischen den Microservices verwendet, um Abhängigkeiten zwischen den Klassen zu analysieren.
Im Zuge dessen werden die Klassen überarbeitet, die abhängig von den Klassen anderer Microservices sind.
Dabei werden für Kommunikation zwischen verschiedenen Microservices lokale Funktionsaufrufe durch \glspl{api} und Aufrufe auf diese ersetzt.

Um diese Methode beziehungsweise das Werkzeug zu validieren, wurde ein Experiment mit zehn passenden, zufällig ausgewählten GitHub-Projekten durchgeführt.
Die Verwendung von \emph{MicroRefact} zur Umstrukturierung dieser Projekte zu Microservices-Applikationen führte zu einer Erfolgsquote von 80\%.
Es wurde jedoch nur die technische Durchführbarkeit bestätigt und nicht überprüft, ob die neue Architektur in irgendeiner Weise vorteilhaft für die Systeme ist.

% TODO abbildung?
% Copyright © 2021 ACM
%
%Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org

\subsubsection{Ergebnis 5: \citetitle{arh-result-no-filter-5} \cite{arh-result-no-filter-5}}

In diesem Artikel beschreiben \citeauthor{arh-result-no-filter-5} eine systematische Methode zur Dekomposition von Monolithen in Microservices mit einer eingeschränkten Umsetzung im zugehörigen Werkzeug \emph{MonoBreaker}.
Dabei wird durch eine statische und dynamische Analyse des Systems ein Graph der Komponenten des Systems aufgebaut.
Die gewichteten Kanten bilden die Stärke der Abhängigkeiten zwischen den Komponenten ab.
Dieser Graph kann dann durch einen Clustering-Algorithmus gruppiert werden, um stark abhängige Komponenten zu einem Microservice zusammenzufassen.
Dabei sollten sie für geringes \emph{Coupling} möglichst schwach mit anderen Komponenten verbunden sein. 
Es wird nicht festgelegt, welcher Algorithmus dafür verwendet werden soll.

In einer anschließenden industriellen Fallstudie wurde \emph{MonoBreaker} an einer Webapplikation getestet.
Die Entwickler dieser Anwendung wurden zu der vorgeschlagenen Dekomposition befragt.
Das Ergebnis des Fragebogens war eine positive Bewertung der erhaltenen Dekomposition sowie eine bessere Bewertung der Nutzung von dynamischer und statischer Analyse statt nur statischer.

% TODO Übersicht, TODO get license under "Rights and permissions" https://link.springer.com/chapter/10.1007/978-3-030-58923-3_21

\subsubsection{Ergebnis 6: \citetitle{arh-result-important-filter-4} \cite{arh-result-important-filter-4}}

\citeauthor{arh-result-important-filter-4} beschreiben eine Methode zur Erkennung von Microservices in großen, monolithischen Systemen, die auf \gls{iiot} Knoten laufen können.
Dabei wird sowohl semantische Kenntnis des Systems sowie syntaktisches Wissen über den Code verwendet.
Im ersten Schritt werden durch die Analyse von \gls{sql}-Queries und Datenbankschemas \glspl{bo} abgeleitet.
Im zweiten Schritt werden semantische und strukturelle Beziehungen durch Analyse der \gls{bo}- und Methodenbeziehungen identifiziert.
In den folgenden zwei Schritten wird außerdem syntaktische Analyse angewendet, wobei strukturelle Ähnlichkeiten von Methoden sowie Interaktionen zwischen diesen analysiert werden.
Im fünften Schritt wird auf Basis der nun bekannten Details über das System ein \emph{$k$-Means Clustering Algorithmus} angewendet, der letztendlich eine Aufteilung in $k$ Microservices vorschlägt.

Die Methode wurde in einer Fallstudie getestet.
Dabei wurde das quelloffene professionelle System \emph{Dolibarr} zu einer \gls{msa} umstrukturiert.
Es konnten Erfolge in Performance und Effizienz gemessen werden.

\subsubsection{Ergebnis 7: \citetitle{arh-result-important-filter-7} \cite{arh-result-important-filter-7}}

\citeauthor{arh-result-important-filter-7} stellen das \emph{Mulit-Objective}-Optimierungsproblem-basierte Verfahren \emph{toMicroservices} vor.
Als Ziele des Problems definieren sie die Kriterien \emph{Coupling}, \emph{Cohesion}, \emph{Feature Modularization}, \emph{Network Overhead} und \emph{Reuse}.
Die Eingabe wird dabei auf Funktionsebene erwartet, beispielsweise durch ein Klassendiagramm.
Dazu gehört ebenfalls eine Liste von Features mit Verweisen auf die Stellen, an denen sie implementiert sind.
Außerdem muss eine Nummer für die Anzahl an erwarteten Microservices angegeben werden.
Mit dem daraus entstehendem Eingabe-Graphen wird durch einen auf dem modernem evolutionären Algorithmus \emph{NSGA-III} \cite{NSGA-III} basierenden Algorithmus eine Menge an möglichen Lösungen berechnet.
Eine mögliche Lösung besteht aus einer Liste von Microservices, wobei jeder Microservice eine Liste von Funktionen beinhaltet.
Es wird hervorgehoben, dass das Ergebnis nicht direkt zur Umgestaltung verwendet werden kann, sondern nur als Vorlage und Startpunkt für eine mögliche Umgestaltung dienen soll.
 
In der durchgeführten Fallstudie zeigen die Autoren, dass alle mit ihrem Ansatz produzierten Microservices-Kandidaten bessere Ergebnisse liefert als die Durchführung mit \emph{Random Search}.

\subsubsection{Vergleich}

Nach Betrachtung der sechs beschriebenen Ergebnisse werden diese nun verglichen, um eine geeignete Methode für das weitere Vorgehen auszuwählen.
Es ist dabei möglich, das generelle Prinzip einer oder mehrerer dieser Methoden zu verwenden, ohne sie komplett originalgetreu umzusetzen.

Zu Beginn des Vergleichs werden direkt zwei Methoden ausgeschlossen, da sie offensichtlich nicht infrage kommen, um den Vergleich übersichtlicher zu gestalten. 
Ergebnis 4, die Methode von \Citet{arh-result-no-filter-4} ermöglicht die automatische Umgestaltung eines Java-Monolithen in Microservices unter Angabe einer Dekomposition.

Das Werkzeug setzt eine bereits vorgegebene Dekomposition in Microservices als Eingabe voraus und führt lediglich die technische Umwandlung des Monolithen in Microservices durch. 
Das Ziel dieser Arbeit besteht jedoch darin, eine geeignete Dekomposition mit optimaler Service-Granularität zu finden. 
Da die Methode dieses Ziel nicht behandelt, kann sie ausgeschlossen werden und wird zur Vereinfachung nachfolgend nicht aufgeführt.

Aus ähnlichem Grund wird auch Ergebnis 1, die Methode von \Citet{arh-result-no-filter-1}, ausgeschlossen.
Die Methode schlägt lediglich einen sehr abstrakten und allgemeinen Plan vor, nach dem bei der Migration mit den Transformations-Regelsammlungen vorgegangen werden kann.
Diese Regeln sind nur sehr abstrakt und deswegen wenig hilfreich.
Eine der Regeln bedeutet beispielsweise das Folgende:
Solange das System ein Legacy-System ist, sollen Microservices erstellt werden, bis die Modularität und Skalierbarkeit maximal sind.
Es fehlt also auch hier ein konkreter \gls{sia}, weswegen diese Methode ebenfalls ausgeschlossen und nicht weiter betrachtet wird.

Wie auch in der Übersicht über die restlichen Verfahren in \cref{tab:phase2-comparison} werden diese und die Anwendbarkeit dieser auf \jf im Folgenden nach Bewertung des Autors und \gls{po} geordnet (\cref{feldnotiz:7,feldnotiz:11}) diskutiert.

\input{tables/phase2-method-comparison}

Das favorisierte Verfahren ist Ergebnis 3, die Methode \gls{mb} von \Citet{arh-result-no-filter-2}.
Ein großer Vorteil gegenüber den anderen Verfahren ist, dass die Ausgangslage einer vorhandenen \gls{msa} zu \emph{jadice flow} passt.
Dadurch können Umwege in der Methodik zur Behandlung der Ausnahme einer bereits vorhandenen \gls{msa}, wo Monolithen als Startpunkt vorgesehen sind, vermieden werden.
Außerdem ist die Eingabe von User Stories praktisch, da diese bei \emph{jadice flow} bereits vorliegen.
Des Weiteren bietet die Methode große Flexibilität in der Wahl des Algorithmus zur Gruppierung zu Microservices.
Es kann zwischen einem genetischen Algorithmus, einem semantischem Gruppierungs-Algorithmus und manuellem Eingreifen gewählt werden.
Zudem bietet \gls{mb} eine Visualisierung der Gruppierung, in der die berechneten Metriken der einzelnen Microservices angezeigt werden können.

Bei der Bewertung der weiteren Ergebnisse wurden weniger Präferenzen für die Funktionsweise der Methode berücksichtigt, sondern vielmehr inwiefern sie überhaupt sinnvoll und mit adäquatem Zeitaufwand anwendbar sind.
Den zweiten Platz belegt das Verfahren von \Citet{arh-result-no-filter-3} (Ergebnis 2).
Dabei werden \glspl{bp} als Eingabe erwartet, welche erst für \jf erstellt werden müssten.
Der dafür nötige Zeitaufwand wird allerdings als überschaubar eingeschätzt und die Methode somit als anwendbar bewertet. 

Das nächste Ergebnis in Bewertungsreihenfolge ist \emph{MonoBreaker} (Ergebnis 5 \cite{arh-result-no-filter-5}).
Im Kontext dieser Thesis ist \emph{MonoBreaker} nicht anwendbar, da es einen Python-Monolithen mit Django Framework voraussetzt.
Es wäre jedoch potenziell möglich, die generelle Methodik zu verwenden oder je nach Struktur des Werkzeugs \emph{MonoBreaker} für einen Teil des Prozesses zu nutzen.
Es ist allerdings fraglich, ob der erhöhte Aufwand, der sich aus der manuellen Umsetzung der Methode ergibt, sinnvoll ist.
Es müsste entweder ein passendes Werkzeug für die statische und dynamische Analyse von \jf gefunden werden oder diese Analysen komplett manuell durchgeführt werden.

Abgesehen von den ausgeschlossenen beiden Methoden wird damit Ergebnis 6 von \Citet{arh-result-important-filter-4} als am wenigsten passend bewertet.
Die Methode basiert zum einen auf der Analyse von \gls{sql}-Abfragen und Datenbank-Schemata.
Im Kontext von \jf würde das nicht gut funktionieren, da die Datenbank nicht die eigentlichen \glspl{bo} abbildet.
Zum anderen ist das Ziel dieser Methode, Microservices für das Betreiben auf \gls{iiot}-Knoten zu identifizieren.
Es ist unklar, inwiefern sich die Ergebnisse von den gewünschten Ergebnissen unterscheiden, wenn kein \gls{iiot} verwendet wird.

\subsection{Planung der Dekompositionsmethodik}

\section{Phase 3a - Architekturplanung}
\label{sec:durchführung-phase3}
